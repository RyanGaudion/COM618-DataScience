{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Inspired by https://www.kaggle.com/code/mahmoudlimam/resampling-tutorial\n",
    "# Importance measurement - f1 score (recall & precision)\n",
    "\n",
    "y_col = \"HeartDiseaseOrAttack\"\n",
    "colsToDrop = [\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/heart_disease_health_indicators_BRFSS2021.csv')\n",
    "\n",
    "for col in colsToDrop:\n",
    "    df = df.drop(col, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: xlabel='HeartDiseaseOrAttack', ylabel='count'>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df, x=y_col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X = df.drop(y_col, axis=1)\n",
    "y = df[y_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_evaluate(X_train, X_test, y_train, y_test, sampler=None):\n",
    "    model = RandomForestClassifier(max_depth=6, n_jobs=-1, n_estimators=50, verbose=1)\n",
    "\n",
    "    Xs_train, ys_train = X_train, y_train\n",
    "\n",
    "    if sampler is not None:\n",
    "        Xs_train, ys_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    model.fit(Xs_train, ys_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"Training Results:\\n\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print(\"\\n\\nTesting Results:\\n\")\n",
    "    print(classification_report(y_test, y_pred_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      1.00      0.96    194844\n",
      "         1.0       0.75      0.00      0.01     18283\n",
      "\n",
      "    accuracy                           0.91    213127\n",
      "   macro avg       0.83      0.50      0.48    213127\n",
      "weighted avg       0.90      0.91      0.87    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96     21713\n",
      "         1.0       0.67      0.00      0.00      1968\n",
      "\n",
      "    accuracy                           0.92     23681\n",
      "   macro avg       0.79      0.50      0.48     23681\n",
      "weighted avg       0.90      0.92      0.88     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without re-sampling\n",
    "train_evaluate(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# With no re-sampling we get for test:\n",
    "# Recall for 1.0 - 0.01\n",
    "# Macro-Average F1 - 0.48"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83    194844\n",
      "         1.0       0.22      0.83      0.34     18283\n",
      "\n",
      "    accuracy                           0.73    213127\n",
      "   macro avg       0.60      0.77      0.58    213127\n",
      "weighted avg       0.91      0.73      0.79    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83     21713\n",
      "         1.0       0.21      0.83      0.34      1968\n",
      "\n",
      "    accuracy                           0.73     23681\n",
      "   macro avg       0.59      0.77      0.58     23681\n",
      "weighted avg       0.92      0.73      0.79     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## -- Undersampling --\n",
    "\n",
    "#Random Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=11)\n",
    "\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.83\n",
    "# Macro-Average F1: 0.48 -> 0.58"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "#https://towardsdatascience.com/k-means-8x-faster-27x-lower-error-than-scikit-learns-in-25-lines-eaedc7a3a0c8\n",
    "# class FaissKMeans:\n",
    "#     def __init__(self, n_clusters=8, n_init=10, max_iter=300):\n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.n_init = n_init\n",
    "#         self.max_iter = max_iter\n",
    "#         self.kmeans = None\n",
    "#         self.cluster_centers_ = None\n",
    "#         self.inertia_ = None\n",
    "#\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "#                                    k=self.n_clusters,\n",
    "#                                    niter=self.max_iter,\n",
    "#                                    nredo=self.n_init)\n",
    "#         self.kmeans.train(X.astype(np.float32))\n",
    "#         self.cluster_centers_ = self.kmeans.centroids\n",
    "#         self.inertia_ = self.kmeans.obj[-1]\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         return self.kmeans.index.search(X.astype(np.float32), 1)[1]\n",
    "#\n",
    "#     def get_params(self, deep=True):\n",
    "#         out = dict()\n",
    "#         out[\"n_clusters\"] = self.n_clusters\n",
    "#         out[\"n_init\"] = self.n_init\n",
    "#         out[\"max_iter\"] = self.max_iter\n",
    "#         return out\n",
    "#\n",
    "#     def set_params(self, **params):\n",
    "#         for key, value in params.items():\n",
    "#             setattr(self, key, value)\n",
    "#         return self"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\VSCode\\Uni\\COM618-DataScience\\Practice\\01-ReadDataSet\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia for init 1/3: 362594.58290000015\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 359469.9026249998\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 360490.79259999975\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 1/4756: mean batch inertia: 6.864443536376955\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 2/4756: mean batch inertia: 6.933034379272461, ewa inertia: 6.933034379272461\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 3/4756: mean batch inertia: 7.010121390869764, ewa inertia: 6.936275400591997\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 4/4756: mean batch inertia: 6.9612399968652365, ewa inertia: 6.937325003982746\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 5/4756: mean batch inertia: 7.177608512939774, ewa inertia: 6.947427405919547\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 6/4756: mean batch inertia: 6.8025747681442965, ewa inertia: 6.941337268586515\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 7/4756: mean batch inertia: 6.979221368708449, ewa inertia: 6.942930055407829\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 8/4756: mean batch inertia: 6.762280352252054, ewa inertia: 6.935334877865413\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 9/4756: mean batch inertia: 6.9537892598397955, ewa inertia: 6.936110767917167\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 10/4756: mean batch inertia: 6.7493647675725255, ewa inertia: 6.928259279632514\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 11/4756: mean batch inertia: 6.582529700081376, ewa inertia: 6.913723537295359\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 12/4756: mean batch inertia: 6.420579745556463, ewa inertia: 6.8929899596211825\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 13/4756: mean batch inertia: 6.8668506298547065, ewa inertia: 6.891890966116361\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 14/4756: mean batch inertia: 6.654369717258914, ewa inertia: 6.881904699747503\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 15/4756: mean batch inertia: 6.445457703315391, ewa inertia: 6.86355486375083\n",
      "[MiniBatchKMeans] Reassigning 2048 cluster centers.\n",
      "Minibatch step 16/4756: mean batch inertia: 6.426846568576622, ewa inertia: 6.845194041794572\n",
      "[MiniBatchKMeans] Reassigning 1882 cluster centers.\n",
      "Minibatch step 17/4756: mean batch inertia: 6.6015089793421735, ewa inertia: 6.834948626045591\n",
      "Minibatch step 18/4756: mean batch inertia: 6.137892821645725, ewa inertia: 6.805641837831145\n",
      "Minibatch step 19/4756: mean batch inertia: 6.0511412988893225, ewa inertia: 6.773919861824517\n",
      "Minibatch step 20/4756: mean batch inertia: 6.237868286691621, ewa inertia: 6.751382283218504\n",
      "Minibatch step 21/4756: mean batch inertia: 6.308859668927497, ewa inertia: 6.732777005914637\n",
      "Minibatch step 22/4756: mean batch inertia: 5.876355697284763, ewa inertia: 6.696769906115843\n",
      "Minibatch step 23/4756: mean batch inertia: 6.0724886181346855, ewa inertia: 6.6705228260720055\n",
      "Minibatch step 24/4756: mean batch inertia: 5.908774175234653, ewa inertia: 6.638496112799098\n",
      "Minibatch step 25/4756: mean batch inertia: 5.992663187829291, ewa inertia: 6.611342922717995\n",
      "Minibatch step 26/4756: mean batch inertia: 5.79075408819278, ewa inertia: 6.5768423518415\n",
      "Minibatch step 27/4756: mean batch inertia: 5.8499968777176985, ewa inertia: 6.546283096412712\n",
      "Minibatch step 28/4756: mean batch inertia: 5.700771205320398, ewa inertia: 6.510734668627405\n",
      "Minibatch step 29/4756: mean batch inertia: 5.728186181998246, ewa inertia: 6.477833453802975\n",
      "Minibatch step 30/4756: mean batch inertia: 5.729628345150999, ewa inertia: 6.446376160826111\n",
      "Minibatch step 31/4756: mean batch inertia: 5.765843066802896, ewa inertia: 6.417764048089125\n",
      "Minibatch step 32/4756: mean batch inertia: 5.600333713067984, ewa inertia: 6.383396272141612\n",
      "Minibatch step 33/4756: mean batch inertia: 5.752046857918724, ewa inertia: 6.356852022089961\n",
      "Minibatch step 34/4756: mean batch inertia: 5.701671739411558, ewa inertia: 6.32930583473231\n",
      "Minibatch step 35/4756: mean batch inertia: 5.5871952780144, ewa inertia: 6.298104779120756\n",
      "Minibatch step 36/4756: mean batch inertia: 5.585720201895473, ewa inertia: 6.268153512952112\n",
      "Minibatch step 37/4756: mean batch inertia: 5.782790904361293, ewa inertia: 6.247747084819104\n",
      "Minibatch step 38/4756: mean batch inertia: 5.585816735765956, ewa inertia: 6.219917099859555\n",
      "Minibatch step 39/4756: mean batch inertia: 5.612356330602902, ewa inertia: 6.194373011883212\n",
      "Minibatch step 40/4756: mean batch inertia: 5.642015539903578, ewa inertia: 6.171149873437487\n",
      "Minibatch step 41/4756: mean batch inertia: 5.777239724193976, ewa inertia: 6.15458844285111\n",
      "Minibatch step 42/4756: mean batch inertia: 5.598584112058237, ewa inertia: 6.131211977055964\n",
      "Minibatch step 43/4756: mean batch inertia: 5.585543919626474, ewa inertia: 6.108270086186491\n",
      "Minibatch step 44/4756: mean batch inertia: 5.585174835216629, ewa inertia: 6.0862772390724\n",
      "Minibatch step 45/4756: mean batch inertia: 5.5574342188373125, ewa inertia: 6.06404273461108\n",
      "Minibatch step 46/4756: mean batch inertia: 5.391204063054324, ewa inertia: 6.035754123677297\n",
      "Minibatch step 47/4756: mean batch inertia: 5.505648982706721, ewa inertia: 6.013466555021027\n",
      "Minibatch step 48/4756: mean batch inertia: 5.595429863032843, ewa inertia: 5.99589075589471\n",
      "Minibatch step 49/4756: mean batch inertia: 5.526495669140149, ewa inertia: 5.976155661072193\n",
      "Minibatch step 50/4756: mean batch inertia: 5.60114380944453, ewa inertia: 5.9603887843828565\n",
      "Minibatch step 51/4756: mean batch inertia: 5.612086827587897, ewa inertia: 5.9457448898509755\n",
      "Minibatch step 52/4756: mean batch inertia: 5.305954225995898, ewa inertia: 5.91884573863693\n",
      "Minibatch step 53/4756: mean batch inertia: 5.719669611852265, ewa inertia: 5.910471642146797\n",
      "Minibatch step 54/4756: mean batch inertia: 5.485474704517888, ewa inertia: 5.892603208709676\n",
      "Minibatch step 55/4756: mean batch inertia: 5.197100632566691, ewa inertia: 5.863361723920415\n",
      "Minibatch step 56/4756: mean batch inertia: 5.2937614898507395, ewa inertia: 5.839413636376476\n",
      "Minibatch step 57/4756: mean batch inertia: 5.416758831427608, ewa inertia: 5.8216436748063\n",
      "Minibatch step 58/4756: mean batch inertia: 5.306030435973125, ewa inertia: 5.799965398984383\n",
      "Minibatch step 59/4756: mean batch inertia: 5.438725499338035, ewa inertia: 5.7847775457784865\n",
      "Minibatch step 60/4756: mean batch inertia: 5.285333163974949, ewa inertia: 5.763779068138647\n",
      "Minibatch step 61/4756: mean batch inertia: 5.689633686640666, ewa inertia: 5.76066172376116\n",
      "Minibatch step 62/4756: mean batch inertia: 5.268013206695665, ewa inertia: 5.739948969254755\n",
      "Minibatch step 63/4756: mean batch inertia: 5.38229115158838, ewa inertia: 5.7249117199421065\n",
      "Minibatch step 64/4756: mean batch inertia: 5.286115135259127, ewa inertia: 5.706463098618886\n",
      "Minibatch step 65/4756: mean batch inertia: 5.512758528107011, ewa inertia: 5.698319046466492\n",
      "Minibatch step 66/4756: mean batch inertia: 5.232930496832675, ewa inertia: 5.678752401191528\n",
      "Minibatch step 67/4756: mean batch inertia: 5.159257799877776, ewa inertia: 5.656910938624041\n",
      "Minibatch step 68/4756: mean batch inertia: 5.42659174693717, ewa inertia: 5.647227473211539\n",
      "Minibatch step 69/4756: mean batch inertia: 5.252222293323162, ewa inertia: 5.630620003511801\n",
      "Minibatch step 70/4756: mean batch inertia: 5.238026218041614, ewa inertia: 5.614113917697067\n",
      "Minibatch step 71/4756: mean batch inertia: 5.23142023812611, ewa inertia: 5.598024068724575\n",
      "Minibatch step 72/4756: mean batch inertia: 5.214554275849244, ewa inertia: 5.581901589095974\n",
      "Minibatch step 73/4756: mean batch inertia: 5.173391219624218, ewa inertia: 5.564726311584554\n",
      "Minibatch step 74/4756: mean batch inertia: 5.3484020490901685, ewa inertia: 5.555631244437057\n",
      "Minibatch step 75/4756: mean batch inertia: 5.305330468751962, ewa inertia: 5.545107679786118\n",
      "Minibatch step 76/4756: mean batch inertia: 5.204180179493029, ewa inertia: 5.5307738345121775\n",
      "Minibatch step 77/4756: mean batch inertia: 5.1375909080656585, ewa inertia: 5.514242979045268\n",
      "Minibatch step 78/4756: mean batch inertia: 5.212705833270743, ewa inertia: 5.501565249064079\n",
      "Minibatch step 79/4756: mean batch inertia: 5.204173788606363, ewa inertia: 5.489061818932078\n",
      "Minibatch step 80/4756: mean batch inertia: 5.107777440429911, ewa inertia: 5.473031222156745\n",
      "Minibatch step 81/4756: mean batch inertia: 5.342634585367082, ewa inertia: 5.46754886824168\n",
      "Minibatch step 82/4756: mean batch inertia: 5.115963738394848, ewa inertia: 5.452766937046602\n",
      "Minibatch step 83/4756: mean batch inertia: 5.178185867193006, ewa inertia: 5.441222539580716\n",
      "Minibatch step 84/4756: mean batch inertia: 5.111693533545132, ewa inertia: 5.427367928903288\n",
      "Minibatch step 85/4756: mean batch inertia: 5.060177486470863, ewa inertia: 5.411929893006003\n",
      "Minibatch step 86/4756: mean batch inertia: 5.269377178030759, ewa inertia: 5.405936452881406\n",
      "Minibatch step 87/4756: mean batch inertia: 5.11067077863237, ewa inertia: 5.393522398615461\n",
      "Minibatch step 88/4756: mean batch inertia: 5.09909412842301, ewa inertia: 5.3811435518941355\n",
      "Minibatch step 89/4756: mean batch inertia: 5.330157949460785, ewa inertia: 5.378999929757904\n",
      "Minibatch step 90/4756: mean batch inertia: 5.324401873174527, ewa inertia: 5.376704426770756\n",
      "Minibatch step 91/4756: mean batch inertia: 5.2565820469007445, ewa inertia: 5.371654040382113\n",
      "Minibatch step 92/4756: mean batch inertia: 5.193592867312006, ewa inertia: 5.364167694159267\n",
      "Minibatch step 93/4756: mean batch inertia: 5.18050365370388, ewa inertia: 5.356445782796848\n",
      "Minibatch step 94/4756: mean batch inertia: 5.366839368664284, ewa inertia: 5.356882767350858\n",
      "Minibatch step 95/4756: mean batch inertia: 5.137060049362259, ewa inertia: 5.347640612274964\n",
      "Minibatch step 96/4756: mean batch inertia: 5.297466507200935, ewa inertia: 5.345531108470573\n",
      "Minibatch step 97/4756: mean batch inertia: 5.115054726343869, ewa inertia: 5.33584103419419\n",
      "Minibatch step 98/4756: mean batch inertia: 5.174647132477924, ewa inertia: 5.329063850058803\n",
      "Minibatch step 99/4756: mean batch inertia: 5.0152860046664385, ewa inertia: 5.315871476072022\n",
      "Minibatch step 100/4756: mean batch inertia: 5.141651283526857, ewa inertia: 5.3085466187889\n",
      "Minibatch step 101/4756: mean batch inertia: 5.150312012235696, ewa inertia: 5.301893854299774\n",
      "Minibatch step 102/4756: mean batch inertia: 5.258229586122299, ewa inertia: 5.3000580479669965\n",
      "Minibatch step 103/4756: mean batch inertia: 5.187224466125832, ewa inertia: 5.2953141094392295\n",
      "Minibatch step 104/4756: mean batch inertia: 5.021595569316198, ewa inertia: 5.283805975893653\n",
      "Minibatch step 105/4756: mean batch inertia: 5.094887169411579, ewa inertia: 5.275863134852306\n",
      "Minibatch step 106/4756: mean batch inertia: 5.274493000225145, ewa inertia: 5.275805529356319\n",
      "[MiniBatchKMeans] Reassigning 199 cluster centers.\n",
      "Minibatch step 107/4756: mean batch inertia: 5.160504562375922, ewa inertia: 5.270957852887827\n",
      "Minibatch step 108/4756: mean batch inertia: 5.2196694251015305, ewa inertia: 5.2688014988606495\n",
      "Minibatch step 109/4756: mean batch inertia: 5.196803423971789, ewa inertia: 5.265774435145955\n",
      "Minibatch step 110/4756: mean batch inertia: 5.003502899385963, ewa inertia: 5.254747575740039\n",
      "Minibatch step 111/4756: mean batch inertia: 5.103927930517674, ewa inertia: 5.2484065634807475\n",
      "Minibatch step 112/4756: mean batch inertia: 5.194465849620984, ewa inertia: 5.246138697597922\n",
      "Minibatch step 113/4756: mean batch inertia: 5.103839129212856, ewa inertia: 5.240155900686477\n",
      "Minibatch step 114/4756: mean batch inertia: 5.146658219030523, ewa inertia: 5.236224914476281\n",
      "Minibatch step 115/4756: mean batch inertia: 5.257851762285457, ewa inertia: 5.237134186652897\n",
      "Minibatch step 116/4756: mean batch inertia: 5.169997266103819, ewa inertia: 5.234311503734998\n",
      "Minibatch step 117/4756: mean batch inertia: 5.195524337907387, ewa inertia: 5.232680748711981\n",
      "Minibatch step 118/4756: mean batch inertia: 5.170443002110985, ewa inertia: 5.230064045074959\n",
      "Minibatch step 119/4756: mean batch inertia: 5.2363525765448395, ewa inertia: 5.230328438052974\n",
      "Minibatch step 120/4756: mean batch inertia: 5.165092917466197, ewa inertia: 5.227585696978546\n",
      "Minibatch step 121/4756: mean batch inertia: 5.116782959725372, ewa inertia: 5.22292714262212\n",
      "Minibatch step 122/4756: mean batch inertia: 5.004298583899544, ewa inertia: 5.213735194391191\n",
      "Minibatch step 123/4756: mean batch inertia: 5.1002416163638635, ewa inertia: 5.208963507197781\n",
      "Minibatch step 124/4756: mean batch inertia: 5.200922120875979, ewa inertia: 5.208625417758749\n",
      "Minibatch step 125/4756: mean batch inertia: 5.173613502542804, ewa inertia: 5.207153388148294\n",
      "Minibatch step 126/4756: mean batch inertia: 5.198968924893351, ewa inertia: 5.206809283229079\n",
      "Minibatch step 127/4756: mean batch inertia: 5.0451512403061995, ewa inertia: 5.200012584891302\n",
      "Minibatch step 128/4756: mean batch inertia: 5.087259616159337, ewa inertia: 5.195272035634959\n",
      "Minibatch step 129/4756: mean batch inertia: 5.004261016504157, ewa inertia: 5.187241230283425\n",
      "Minibatch step 130/4756: mean batch inertia: 5.08232138119874, ewa inertia: 5.182830014169582\n",
      "Minibatch step 131/4756: mean batch inertia: 5.097103661339388, ewa inertia: 5.179225763188623\n",
      "Minibatch step 132/4756: mean batch inertia: 5.106445548584877, ewa inertia: 5.176165815445371\n",
      "Minibatch step 133/4756: mean batch inertia: 5.158240719721909, ewa inertia: 5.175412178533125\n",
      "Minibatch step 134/4756: mean batch inertia: 5.044924105363659, ewa inertia: 5.169925980296556\n",
      "Minibatch step 135/4756: mean batch inertia: 5.114942576170434, ewa inertia: 5.167614275882272\n",
      "Minibatch step 136/4756: mean batch inertia: 5.123160267911987, ewa inertia: 5.165745265985725\n",
      "Minibatch step 137/4756: mean batch inertia: 4.9182565944325916, ewa inertia: 5.155339932529063\n",
      "Minibatch step 138/4756: mean batch inertia: 5.212422999208586, ewa inertia: 5.157739914474911\n",
      "Minibatch step 139/4756: mean batch inertia: 5.112205174455784, ewa inertia: 5.155825466630539\n",
      "Minibatch step 140/4756: mean batch inertia: 4.9666925844099605, ewa inertia: 5.147873625058259\n",
      "Minibatch step 141/4756: mean batch inertia: 5.180029655325397, ewa inertia: 5.149225582767969\n",
      "Minibatch step 142/4756: mean batch inertia: 5.028782506654752, ewa inertia: 5.144161713130465\n",
      "Minibatch step 143/4756: mean batch inertia: 5.018840922564002, ewa inertia: 5.138892766448126\n",
      "Minibatch step 144/4756: mean batch inertia: 5.124207851781081, ewa inertia: 5.138275358657562\n",
      "Minibatch step 145/4756: mean batch inertia: 5.092981206233657, ewa inertia: 5.136371026000031\n",
      "Minibatch step 146/4756: mean batch inertia: 5.086373427139454, ewa inertia: 5.134268943165646\n",
      "Minibatch step 147/4756: mean batch inertia: 5.165257371453783, ewa inertia: 5.135571810596354\n",
      "Minibatch step 148/4756: mean batch inertia: 5.013599912709481, ewa inertia: 5.13044366367193\n",
      "Minibatch step 149/4756: mean batch inertia: 4.918818532088892, ewa inertia: 5.121546165260741\n",
      "Minibatch step 150/4756: mean batch inertia: 4.843841600378795, ewa inertia: 5.1098704445827\n",
      "Minibatch step 151/4756: mean batch inertia: 5.084539130352205, ewa inertia: 5.108805423021068\n",
      "Minibatch step 152/4756: mean batch inertia: 5.0827955079825795, ewa inertia: 5.1077118705871065\n",
      "Minibatch step 153/4756: mean batch inertia: 5.015893360474992, ewa inertia: 5.103851482920816\n",
      "Minibatch step 154/4756: mean batch inertia: 5.051808822374334, ewa inertia: 5.101663418176035\n",
      "Minibatch step 155/4756: mean batch inertia: 5.127722493441168, ewa inertia: 5.102759037486626\n",
      "Minibatch step 156/4756: mean batch inertia: 5.083440494971657, ewa inertia: 5.101946814949314\n",
      "Minibatch step 157/4756: mean batch inertia: 5.0290464315742645, ewa inertia: 5.098881814869208\n",
      "Minibatch step 158/4756: mean batch inertia: 5.201514575911106, ewa inertia: 5.103196873394986\n",
      "Minibatch step 159/4756: mean batch inertia: 5.04726386388226, ewa inertia: 5.100845244079745\n",
      "Minibatch step 160/4756: mean batch inertia: 5.06618428795211, ewa inertia: 5.099387970079398\n",
      "Minibatch step 161/4756: mean batch inertia: 5.075229496605083, ewa inertia: 5.098372259054217\n",
      "Minibatch step 162/4756: mean batch inertia: 5.044881689776359, ewa inertia: 5.09612331890423\n",
      "Minibatch step 163/4756: mean batch inertia: 5.024385329281271, ewa inertia: 5.093107190130121\n",
      "Minibatch step 164/4756: mean batch inertia: 4.928466643767421, ewa inertia: 5.08618509638482\n",
      "Minibatch step 165/4756: mean batch inertia: 5.155256057263489, ewa inertia: 5.089089093467209\n",
      "Minibatch step 166/4756: mean batch inertia: 5.051140150468457, ewa inertia: 5.087493580413008\n",
      "Minibatch step 167/4756: mean batch inertia: 5.066772294118214, ewa inertia: 5.086622381371067\n",
      "Minibatch step 168/4756: mean batch inertia: 5.047640547430997, ewa inertia: 5.084983441785052\n",
      "Minibatch step 169/4756: mean batch inertia: 5.018486577300307, ewa inertia: 5.082187669176779\n",
      "Minibatch step 170/4756: mean batch inertia: 5.005812878013229, ewa inertia: 5.078976592222216\n",
      "Minibatch step 171/4756: mean batch inertia: 5.039051112385065, ewa inertia: 5.077297978294089\n",
      "Minibatch step 172/4756: mean batch inertia: 4.966900712966452, ewa inertia: 5.072656471467822\n",
      "Minibatch step 173/4756: mean batch inertia: 5.180624844046674, ewa inertia: 5.077195858714947\n",
      "Minibatch step 174/4756: mean batch inertia: 5.048271573916103, ewa inertia: 5.075979775463788\n",
      "Minibatch step 175/4756: mean batch inertia: 4.965023085746768, ewa inertia: 5.0713147483798915\n",
      "Minibatch step 176/4756: mean batch inertia: 4.954732727976651, ewa inertia: 5.0664132117166805\n",
      "Minibatch step 177/4756: mean batch inertia: 4.9220017957060955, ewa inertia: 5.060341624968451\n",
      "Minibatch step 178/4756: mean batch inertia: 4.924834670066723, ewa inertia: 5.054644414495743\n",
      "Minibatch step 179/4756: mean batch inertia: 5.056523190403452, ewa inertia: 5.054723405140799\n",
      "Minibatch step 180/4756: mean batch inertia: 5.021522383949483, ewa inertia: 5.05332751217152\n",
      "Minibatch step 181/4756: mean batch inertia: 4.827809378611642, ewa inertia: 5.04384590089013\n",
      "Minibatch step 182/4756: mean batch inertia: 5.121790647756004, ewa inertia: 5.04712298455317\n",
      "Minibatch step 183/4756: mean batch inertia: 5.008997801336546, ewa inertia: 5.045520061712397\n",
      "Minibatch step 184/4756: mean batch inertia: 5.093581660335091, ewa inertia: 5.047540747980544\n",
      "Minibatch step 185/4756: mean batch inertia: 4.9961410359551675, ewa inertia: 5.045379715154904\n",
      "Minibatch step 186/4756: mean batch inertia: 5.126643792571148, ewa inertia: 5.048796355675286\n",
      "Minibatch step 187/4756: mean batch inertia: 5.119801831958102, ewa inertia: 5.051781686896045\n",
      "Minibatch step 188/4756: mean batch inertia: 4.935337898644826, ewa inertia: 5.04688596202061\n",
      "Minibatch step 189/4756: mean batch inertia: 4.988262404560583, ewa inertia: 5.044421212179904\n",
      "Minibatch step 190/4756: mean batch inertia: 5.001588046228386, ewa inertia: 5.042620348439623\n",
      "Minibatch step 191/4756: mean batch inertia: 5.0309681776401804, ewa inertia: 5.042130448348837\n",
      "Minibatch step 192/4756: mean batch inertia: 4.9756610892502, ewa inertia: 5.039335832168099\n",
      "Minibatch step 193/4756: mean batch inertia: 5.111469497528712, ewa inertia: 5.0423685966046214\n",
      "Minibatch step 194/4756: mean batch inertia: 4.887656875700815, ewa inertia: 5.035863947177416\n",
      "Minibatch step 195/4756: mean batch inertia: 5.024252754603434, ewa inertia: 5.035375769961854\n",
      "Minibatch step 196/4756: mean batch inertia: 4.981473588968824, ewa inertia: 5.033109524142383\n",
      "[MiniBatchKMeans] Reassigning 64 cluster centers.\n",
      "Minibatch step 197/4756: mean batch inertia: 5.020262604745868, ewa inertia: 5.032569392428989\n",
      "Minibatch step 198/4756: mean batch inertia: 4.921559442995257, ewa inertia: 5.027902126110833\n",
      "Minibatch step 199/4756: mean batch inertia: 4.893203786128505, ewa inertia: 5.022238912781596\n",
      "Minibatch step 200/4756: mean batch inertia: 4.965081578801268, ewa inertia: 5.019835808365435\n",
      "Minibatch step 201/4756: mean batch inertia: 4.903826656941187, ewa inertia: 5.01495835721982\n",
      "Minibatch step 202/4756: mean batch inertia: 4.977820988717051, ewa inertia: 5.013396965740569\n",
      "Minibatch step 203/4756: mean batch inertia: 5.208121285139774, ewa inertia: 5.021583891884521\n",
      "Minibatch step 204/4756: mean batch inertia: 5.0254487631063895, ewa inertia: 5.021746385276959\n",
      "Minibatch step 205/4756: mean batch inertia: 5.1024462138656625, ewa inertia: 5.025139302702598\n",
      "Minibatch step 206/4756: mean batch inertia: 5.011410053757803, ewa inertia: 5.0245620746117785\n",
      "Minibatch step 207/4756: mean batch inertia: 5.171645135023261, ewa inertia: 5.0307459871108975\n",
      "Minibatch step 208/4756: mean batch inertia: 4.906475687146316, ewa inertia: 5.025521206914793\n",
      "Minibatch step 209/4756: mean batch inertia: 4.96502473752495, ewa inertia: 5.0229777129722155\n",
      "Minibatch step 210/4756: mean batch inertia: 5.231943373386991, ewa inertia: 5.031763397439962\n",
      "Minibatch step 211/4756: mean batch inertia: 4.8899125498707905, ewa inertia: 5.025799466401\n",
      "Minibatch step 212/4756: mean batch inertia: 4.921277008464179, ewa inertia: 5.021404958071721\n",
      "Converged (lack of improvement in inertia) at step 212/4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.30      0.46    194844\n",
      "         1.0       0.12      0.98      0.21     18283\n",
      "\n",
      "    accuracy                           0.36    213127\n",
      "   macro avg       0.56      0.64      0.33    213127\n",
      "weighted avg       0.92      0.36      0.44    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.29      0.45     21713\n",
      "         1.0       0.11      0.98      0.20      1968\n",
      "\n",
      "    accuracy                           0.35     23681\n",
      "   macro avg       0.55      0.64      0.33     23681\n",
      "weighted avg       0.92      0.35      0.43     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cluster Centroid Undersampling\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "kmeans = MiniBatchKMeans(verbose=1, batch_size=4096)\n",
    "sampler = ClusterCentroids(estimator=kmeans)\n",
    "\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.98\n",
    "# Macro-Average F1: 0.48 -> 0.33"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83    194844\n",
      "         1.0       0.22      0.83      0.34     18283\n",
      "\n",
      "    accuracy                           0.73    213127\n",
      "   macro avg       0.60      0.77      0.59    213127\n",
      "weighted avg       0.91      0.73      0.79    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.72      0.83     21713\n",
      "         1.0       0.21      0.83      0.34      1968\n",
      "\n",
      "    accuracy                           0.73     23681\n",
      "   macro avg       0.60      0.78      0.59     23681\n",
      "weighted avg       0.92      0.73      0.79     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Oversampling\n",
    "\n",
    "# Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "sampler = RandomOverSampler(random_state=11)\n",
    "\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.83\n",
    "# Macro-Average F1: 0.48 -> 0.59"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90    194844\n",
      "         1.0       0.28      0.60      0.38     18283\n",
      "\n",
      "    accuracy                           0.83    213127\n",
      "   macro avg       0.62      0.73      0.64    213127\n",
      "weighted avg       0.90      0.83      0.86    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91     21713\n",
      "         1.0       0.28      0.61      0.38      1968\n",
      "\n",
      "    accuracy                           0.84     23681\n",
      "   macro avg       0.62      0.73      0.64     23681\n",
      "weighted avg       0.90      0.84      0.86     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SMOTE Oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sampler = SMOTE(k_neighbors=5, random_state=11)\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.61\n",
    "# Macro-Average F1: 0.48 -> 0.64"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91    194844\n",
      "         1.0       0.29      0.60      0.39     18283\n",
      "\n",
      "    accuracy                           0.84    213127\n",
      "   macro avg       0.62      0.73      0.65    213127\n",
      "weighted avg       0.90      0.84      0.86    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91     21713\n",
      "         1.0       0.28      0.59      0.38      1968\n",
      "\n",
      "    accuracy                           0.84     23681\n",
      "   macro avg       0.62      0.73      0.64     23681\n",
      "weighted avg       0.90      0.84      0.86     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Borderline SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "sampler = BorderlineSMOTE(k_neighbors=5, random_state=11)\n",
    "\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.59\n",
    "# Macro-Average F1: 0.48 -> 0.64"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90    194844\n",
      "         1.0       0.28      0.61      0.38     18283\n",
      "\n",
      "    accuracy                           0.83    213127\n",
      "   macro avg       0.62      0.73      0.64    213127\n",
      "weighted avg       0.90      0.83      0.86    213127\n",
      "\n",
      "\n",
      "\n",
      "Testing Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91     21713\n",
      "         1.0       0.28      0.60      0.38      1968\n",
      "\n",
      "    accuracy                           0.84     23681\n",
      "   macro avg       0.62      0.73      0.64     23681\n",
      "weighted avg       0.90      0.84      0.86     23681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from smote_variants import DBSMOTE\n",
    "oversampler = DBSMOTE(min_samples=10)\n",
    "\n",
    "train_evaluate(X_train, X_test, y_train, y_test, sampler)\n",
    "# Test Results:\n",
    "# Recall for 1.0: 0.01 -> 0.6\n",
    "# Macro-Average F1: 0.48 -> 0.64"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
